"""
ì‹¤í—˜ ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ

5ê°€ì§€ ì§„ë‹¨ ë°©ë²• ë¹„êµ ì‹¤í—˜ì„ ìœ„í•œ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤:
- ë¡œê¹… ì„¤ì •
- CSV ì €ì¥/ë¡œë“œ
- ê²°ê³¼ ì •ë¦¬ ë° ë¹„êµ
- íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ ì¶œë ¥ ë””ë ‰í„°ë¦¬ ê´€ë¦¬
"""

import os
import sys
import json
import logging
import csv
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field, asdict
import pandas as pd

# ê²½ë¡œ ì„¤ì •
SCRIPT_DIR = Path(__file__).parent
PROJECT_ROOT = SCRIPT_DIR.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))
sys.path.insert(0, str(SCRIPT_DIR.parent / "eval"))
sys.path.insert(0, str(SCRIPT_DIR.parent / "baseline"))

from ontology_utils import OntologyTree


# ============ ë°ì´í„° í´ë˜ìŠ¤ ============

@dataclass
class AgentToolCall:
    """ì—ì´ì „íŠ¸ ë„êµ¬ í˜¸ì¶œ ê¸°ë¡"""
    tool_name: str
    tool_input: Dict[str, Any]
    tool_output: str
    timestamp: str = ""

    def to_dict(self) -> Dict:
        return asdict(self)


@dataclass
class AgentStep:
    """ì—ì´ì „íŠ¸ ë‹¨ì¼ ì¶”ë¡  ë‹¨ê³„"""
    step_num: int
    thought: str = ""
    action: str = ""
    action_input: Dict[str, Any] = field(default_factory=dict)
    observation: str = ""
    tool_calls: List[AgentToolCall] = field(default_factory=list)

    def to_dict(self) -> Dict:
        result = {
            "step_num": self.step_num,
            "thought": self.thought,
            "action": self.action,
            "action_input": self.action_input,
            "observation": self.observation[:1000] if self.observation else "",  # ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
        }
        if self.tool_calls:
            result["tool_calls"] = [tc.to_dict() for tc in self.tool_calls]
        return result


@dataclass
class AgentTrace:
    """ì—ì´ì „íŠ¸ ì „ì²´ ì¶”ë¡  íŠ¸ë ˆì´ìŠ¤"""
    sample_id: int
    filename: str
    agent_type: str  # "dermatology_agent" or "react_agent"

    # ì…ë ¥
    image_path: str = ""

    # ì¶”ë¡  ê³¼ì •
    steps: List[AgentStep] = field(default_factory=list)

    # ê´€ì°° ê²°ê³¼ (ì´ˆê¸° ì´ë¯¸ì§€ ë¶„ì„)
    observations: Dict[str, Any] = field(default_factory=dict)

    # ì˜¨í†¨ë¡œì§€ íƒìƒ‰ ê²½ë¡œ
    ontology_path: List[str] = field(default_factory=list)
    explored_categories: List[str] = field(default_factory=list)

    # í›„ë³´êµ°
    candidates_considered: List[str] = field(default_factory=list)
    candidate_scores: Dict[str, float] = field(default_factory=dict)

    # ìµœì¢… ê²°ê³¼
    primary_diagnosis: str = ""
    differential_diagnoses: List[str] = field(default_factory=list)
    confidence: float = 0.0
    final_reasoning: str = ""

    # ë©”íƒ€ ì •ë³´
    total_steps: int = 0
    total_vlm_calls: int = 0
    errors: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)

    def to_dict(self) -> Dict:
        return {
            "sample_id": self.sample_id,
            "filename": self.filename,
            "agent_type": self.agent_type,
            "image_path": self.image_path,
            "steps": [s.to_dict() for s in self.steps],
            "observations": self.observations,
            "ontology_path": self.ontology_path,
            "explored_categories": self.explored_categories,
            "candidates_considered": self.candidates_considered[:20],  # ìƒìœ„ 20ê°œë§Œ
            "candidate_scores": dict(list(self.candidate_scores.items())[:20]),
            "primary_diagnosis": self.primary_diagnosis,
            "differential_diagnoses": self.differential_diagnoses,
            "confidence": self.confidence,
            "final_reasoning": self.final_reasoning[:500] if self.final_reasoning else "",
            "total_steps": self.total_steps,
            "total_vlm_calls": self.total_vlm_calls,
            "errors": self.errors,
            "warnings": self.warnings,
        }

    def to_readable_text(self) -> str:
        """ê°€ë…ì„± ì¢‹ì€ í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        lines = []
        lines.append("=" * 80)
        lines.append(f"[Sample {self.sample_id}] {self.filename}")
        lines.append(f"Agent: {self.agent_type}")
        lines.append("=" * 80)

        # ê´€ì°° ê²°ê³¼
        if self.observations:
            lines.append("\nğŸ“· [Initial Observations]")
            for key, value in self.observations.items():
                if isinstance(value, list):
                    lines.append(f"  â€¢ {key}: {', '.join(str(v) for v in value)}")
                else:
                    lines.append(f"  â€¢ {key}: {value}")

        # ì¶”ë¡  ë‹¨ê³„
        if self.steps:
            lines.append(f"\nğŸ”„ [Reasoning Steps] ({len(self.steps)} steps)")
            for step in self.steps:
                lines.append(f"\n  --- Step {step.step_num} ---")
                if step.thought:
                    lines.append(f"  ğŸ’­ Thought: {step.thought[:200]}...")
                if step.action:
                    lines.append(f"  ğŸ”§ Action: {step.action}")
                if step.action_input:
                    lines.append(f"     Input: {json.dumps(step.action_input, ensure_ascii=False)[:200]}")
                if step.observation:
                    obs_preview = step.observation[:300].replace('\n', ' ')
                    lines.append(f"  ğŸ“‹ Observation: {obs_preview}...")

        # ì˜¨í†¨ë¡œì§€ ê²½ë¡œ
        if self.ontology_path:
            lines.append(f"\nğŸŒ³ [Ontology Path]")
            lines.append(f"  {' â†’ '.join(self.ontology_path)}")

        # í›„ë³´êµ°
        if self.candidates_considered:
            lines.append(f"\nğŸ¯ [Candidates] ({len(self.candidates_considered)} considered)")
            top_candidates = self.candidates_considered[:5]
            for cand in top_candidates:
                score = self.candidate_scores.get(cand, 0)
                lines.append(f"  â€¢ {cand} (score: {score:.2f})")

        # ìµœì¢… ê²°ê³¼
        lines.append(f"\nâœ… [Final Diagnosis]")
        lines.append(f"  Primary: {self.primary_diagnosis}")
        lines.append(f"  Confidence: {self.confidence:.2f}")
        if self.differential_diagnoses:
            lines.append(f"  Differentials: {', '.join(self.differential_diagnoses[:3])}")
        if self.final_reasoning:
            lines.append(f"  Reasoning: {self.final_reasoning[:300]}...")

        # ë©”íƒ€ ì •ë³´
        lines.append(f"\nğŸ“Š [Stats]")
        lines.append(f"  Total Steps: {self.total_steps}")
        lines.append(f"  VLM Calls: {self.total_vlm_calls}")
        if self.errors:
            lines.append(f"  âš ï¸ Errors: {len(self.errors)}")
        if self.warnings:
            lines.append(f"  âš ï¸ Warnings: {len(self.warnings)}")

        lines.append("\n" + "=" * 80 + "\n")
        return "\n".join(lines)


@dataclass
class MethodResult:
    """ë‹¨ì¼ ë°©ë²•ì˜ ë‹¨ì¼ ìƒ˜í”Œ ê²°ê³¼"""
    sample_id: int
    filename: str
    ground_truth: str
    hierarchical_gt: str
    prediction: str  # ì£¼ìš” ì˜ˆì¸¡ (Top-1)
    confidence: float = 0.0
    reasoning: str = ""
    raw_response: str = ""
    all_predictions: List[str] = field(default_factory=list)  # ì „ì²´ ì˜ˆì¸¡ ë¦¬ìŠ¤íŠ¸ (Top-Kìš©)
    agent_trace: Optional[AgentTrace] = None  # ì—ì´ì „íŠ¸ ìƒì„¸ íŠ¸ë ˆì´ìŠ¤

    def to_dict(self) -> Dict:
        result = asdict(self)
        # agent_traceëŠ” ë³„ë„ ì²˜ë¦¬ (ë„ˆë¬´ í¬ë¯€ë¡œ ê¸°ë³¸ dictì—ì„œ ì œì™¸)
        if 'agent_trace' in result:
            del result['agent_trace']
        return result


@dataclass
class MethodEvaluation:
    """ë‹¨ì¼ ë°©ë²•ì˜ ì „ì²´ í‰ê°€ ê²°ê³¼"""
    method_name: str
    exact_match: float = 0.0
    partial_match: float = 0.0
    hierarchical_f1: float = 0.0
    avg_distance: float = 0.0
    partial_credit: float = 0.0
    level_accuracy: Dict[int, float] = field(default_factory=dict)
    total_samples: int = 0
    valid_samples: int = 0
    # Top-K ë©”íŠ¸ë¦­ ì¶”ê°€
    top_k_accuracy: Dict[int, float] = field(default_factory=dict)  # {k: accuracy}
    top_k_hierarchical_f1: Dict[int, float] = field(default_factory=dict)  # {k: h_f1}

    def to_dict(self) -> Dict:
        return asdict(self)


@dataclass
class ExperimentConfig:
    """ì‹¤í—˜ ì„¤ì •"""
    timestamp: str
    input_csv: str
    output_dir: str
    model: str
    num_samples: int
    test_mode: bool
    methods: List[str]

    def to_dict(self) -> Dict:
        return asdict(self)


# ============ ë¡œê¹… ì„¤ì • ============

def setup_logging(output_dir: Path, name: str = "experiment") -> logging.Logger:
    """
    ì‹¤í—˜ ë¡œê¹… ì„¤ì •

    Args:
        output_dir: ë¡œê·¸ íŒŒì¼ ì €ì¥ ë””ë ‰í„°ë¦¬
        name: ë¡œê±° ì´ë¦„

    Returns:
        ì„¤ì •ëœ ë¡œê±°
    """
    # ë¡œê·¸ ë””ë ‰í„°ë¦¬ ìƒì„±
    log_dir = output_dir / "logs"
    log_dir.mkdir(parents=True, exist_ok=True)

    # ë¡œê±° ìƒì„±
    logger = logging.getLogger(name)
    logger.setLevel(logging.INFO)

    # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±°
    logger.handlers = []

    # íŒŒì¼ í•¸ë“¤ëŸ¬ (í•œêµ­ì–´ ì§€ì›)
    log_file = log_dir / f"{name}.log"
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setLevel(logging.INFO)
    file_formatter = logging.Formatter(
        '%(asctime)s [%(levelname)s] %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    file_handler.setFormatter(file_formatter)
    logger.addHandler(file_handler)

    # ì½˜ì†” í•¸ë“¤ëŸ¬
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_formatter = logging.Formatter('[%(levelname)s] %(message)s')
    console_handler.setFormatter(console_formatter)
    logger.addHandler(console_handler)

    return logger


# ============ ì¶œë ¥ ë””ë ‰í„°ë¦¬ ê´€ë¦¬ ============

def create_output_directory(base_dir: str, test_mode: bool = False) -> Path:
    """
    íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ ì¶œë ¥ ë””ë ‰í„°ë¦¬ ìƒì„±

    Args:
        base_dir: ê¸°ë³¸ ì¶œë ¥ ë””ë ‰í„°ë¦¬
        test_mode: í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì—¬ë¶€

    Returns:
        ìƒì„±ëœ ì¶œë ¥ ë””ë ‰í„°ë¦¬ ê²½ë¡œ
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    suffix = "_test" if test_mode else ""
    output_dir = Path(base_dir) / f"{timestamp}{suffix}"

    # í•˜ìœ„ ë””ë ‰í„°ë¦¬ ìƒì„±
    (output_dir / "logs").mkdir(parents=True, exist_ok=True)
    (output_dir / "predictions").mkdir(parents=True, exist_ok=True)
    (output_dir / "evaluation").mkdir(parents=True, exist_ok=True)

    return output_dir


# ============ CSV ì €ì¥ í•¨ìˆ˜ ============

def save_predictions_csv(
    results: List[MethodResult],
    output_path: Path,
    method_name: str
) -> None:
    """
    ì˜ˆì¸¡ ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥

    Args:
        results: ì˜ˆì¸¡ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        output_path: ì €ì¥ ê²½ë¡œ
        method_name: ë°©ë²• ì´ë¦„
    """
    df = pd.DataFrame([r.to_dict() for r in results])
    df.to_csv(output_path, index=False, encoding='utf-8-sig')


def save_metrics_summary_csv(
    evaluations: Dict[str, MethodEvaluation],
    output_path: Path
) -> None:
    """
    ë©”íŠ¸ë¦­ ìš”ì•½ì„ CSVë¡œ ì €ì¥

    Args:
        evaluations: {method_name: MethodEvaluation} ë”•ì…”ë„ˆë¦¬
        output_path: ì €ì¥ ê²½ë¡œ
    """
    rows = []
    for method_name, eval_result in evaluations.items():
        row = {
            'method': method_name,
            'exact_match': eval_result.exact_match,
            'partial_match': eval_result.partial_match,
            'hierarchical_f1': eval_result.hierarchical_f1,
            'avg_distance': eval_result.avg_distance,
            'partial_credit': eval_result.partial_credit,
            'total_samples': eval_result.total_samples,
            'valid_samples': eval_result.valid_samples,
        }

        # ë ˆë²¨ë³„ ì •í™•ë„ ì¶”ê°€
        for level, acc in eval_result.level_accuracy.items():
            row[f'level_{level}_acc'] = acc

        # Top-K ì •í™•ë„ ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
        if eval_result.top_k_accuracy:
            for k, acc in eval_result.top_k_accuracy.items():
                row[f'top_{k}_accuracy'] = acc

        # Top-K Hierarchical F1 ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
        if eval_result.top_k_hierarchical_f1:
            for k, f1 in eval_result.top_k_hierarchical_f1.items():
                row[f'top_{k}_h_f1'] = f1

        rows.append(row)

    df = pd.DataFrame(rows)
    df.to_csv(output_path, index=False, encoding='utf-8-sig')


def save_per_sample_comparison_csv(
    all_results: Dict[str, List[MethodResult]],
    evaluator,  # HierarchicalEvaluator
    output_path: Path
) -> None:
    """
    ìƒ˜í”Œë³„ ë¹„êµ CSV ì €ì¥

    Args:
        all_results: {method_name: [MethodResult, ...]} ë”•ì…”ë„ˆë¦¬
        evaluator: HierarchicalEvaluator ì¸ìŠ¤í„´ìŠ¤
        output_path: ì €ì¥ ê²½ë¡œ
    """
    # ë°©ë²• ì´ë¦„ ì •ë ¬
    method_names = sorted(all_results.keys())

    if not method_names or not all_results[method_names[0]]:
        return

    # ìƒ˜í”Œ ìˆ˜
    num_samples = len(all_results[method_names[0]])

    rows = []
    for i in range(num_samples):
        row = {
            'sample_id': i,
            'filename': all_results[method_names[0]][i].filename,
            'ground_truth': all_results[method_names[0]][i].ground_truth,
            'hierarchical_gt': all_results[method_names[0]][i].hierarchical_gt,
        }

        # ê° ë°©ë²•ë³„ ê²°ê³¼ ì¶”ê°€
        for j, method in enumerate(method_names, 1):
            result = all_results[method][i]
            pred = result.prediction
            gt = result.ground_truth

            # ì˜ˆì¸¡ê°’ì´ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ë¬¸ìì—´ë¡œ ë³€í™˜
            if isinstance(pred, list):
                pred = pred[0] if pred else ""
            if isinstance(gt, list):
                gt = gt[0] if gt else ""

            # ê±°ë¦¬ ê³„ì‚°
            try:
                dist = evaluator.tree.get_hierarchical_distance(gt, pred) if pred else -1
            except Exception:
                dist = -1
            exact = 1 if gt == pred else 0

            row[f'm{j}_pred'] = pred
            row[f'm{j}_exact'] = exact
            row[f'm{j}_dist'] = dist
            row[f'm{j}_conf'] = result.confidence

        # ìµœì„ ì˜ ë°©ë²• ê²°ì •
        best_methods = []
        min_dist = float('inf')
        for j, method in enumerate(method_names, 1):
            dist = row.get(f'm{j}_dist', -1)
            if dist >= 0 and dist < min_dist:
                min_dist = dist
                best_methods = [f'm{j}']
            elif dist >= 0 and dist == min_dist:
                best_methods.append(f'm{j}')

        row['best_method'] = '/'.join(best_methods) if best_methods else 'none'
        rows.append(row)

    df = pd.DataFrame(rows)
    df.to_csv(output_path, index=False, encoding='utf-8-sig')


def save_detailed_analysis_csv(
    all_results: Dict[str, List[MethodResult]],
    evaluator,  # HierarchicalEvaluator
    output_path: Path
) -> None:
    """
    ìƒì„¸ ë¶„ì„ CSV ì €ì¥ (ê° ìƒ˜í”Œ-ë°©ë²• ì¡°í•©ë³„ í•œ í–‰)

    Args:
        all_results: {method_name: [MethodResult, ...]} ë”•ì…”ë„ˆë¦¬
        evaluator: HierarchicalEvaluator ì¸ìŠ¤í„´ìŠ¤
        output_path: ì €ì¥ ê²½ë¡œ
    """
    rows = []

    for method_name, results in all_results.items():
        for result in results:
            gt = result.ground_truth
            pred = result.prediction

            # ì˜ˆì¸¡ê°’ì´ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ë¬¸ìì—´ë¡œ ë³€í™˜
            if isinstance(pred, list):
                pred = pred[0] if pred else ""
            if isinstance(gt, list):
                gt = gt[0] if gt else ""

            # ê³„ì¸µì  ìœ ì‚¬ë„ ê³„ì‚°
            if pred and gt:
                try:
                    similarity = evaluator.hierarchical_similarity(gt, pred)
                    distance = evaluator.tree.get_hierarchical_distance(gt, pred)
                    lca = evaluator.tree.get_lca(gt, pred)
                except Exception:
                    similarity = 0.0
                    distance = -1
                    lca = ""
            else:
                similarity = 0.0
                distance = -1
                lca = ""

            row = {
                'sample_id': result.sample_id,
                'filename': result.filename,
                'ground_truth': gt,
                'method': method_name,
                'prediction': pred,
                'exact_match': 1 if gt == pred else 0,
                'hierarchical_similarity': round(similarity, 4),
                'tree_distance': distance,
                'common_ancestor': lca or "",
                'confidence': result.confidence,
                'reasoning_summary': result.reasoning[:200] if result.reasoning else ""
            }
            rows.append(row)

    df = pd.DataFrame(rows)
    df.to_csv(output_path, index=False, encoding='utf-8-sig')


def save_experiment_config(config: ExperimentConfig, output_path: Path) -> None:
    """
    ì‹¤í—˜ ì„¤ì •ì„ JSONìœ¼ë¡œ ì €ì¥

    Args:
        config: ì‹¤í—˜ ì„¤ì •
        output_path: ì €ì¥ ê²½ë¡œ
    """
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(config.to_dict(), f, indent=2, ensure_ascii=False)


# ============ ì—ì´ì „íŠ¸ íŠ¸ë ˆì´ìŠ¤ ì €ì¥ í•¨ìˆ˜ ============

def save_agent_traces_json(
    results: List[MethodResult],
    output_path: Path,
    method_name: str
) -> None:
    """
    ì—ì´ì „íŠ¸ íŠ¸ë ˆì´ìŠ¤ë¥¼ JSONìœ¼ë¡œ ì €ì¥ (êµ¬ì¡°í™”ëœ í˜•ì‹)

    Args:
        results: MethodResult ë¦¬ìŠ¤íŠ¸ (agent_trace í¬í•¨)
        output_path: ì €ì¥ ê²½ë¡œ
        method_name: ë°©ë²• ì´ë¦„
    """
    traces = []
    for result in results:
        if result.agent_trace:
            trace_dict = result.agent_trace.to_dict()
            trace_dict["ground_truth"] = result.ground_truth
            trace_dict["hierarchical_gt"] = result.hierarchical_gt
            traces.append(trace_dict)

    if traces:
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump({
                "method": method_name,
                "total_samples": len(traces),
                "traces": traces
            }, f, indent=2, ensure_ascii=False)


def save_agent_traces_readable(
    results: List[MethodResult],
    output_path: Path,
    method_name: str
) -> None:
    """
    ì—ì´ì „íŠ¸ íŠ¸ë ˆì´ìŠ¤ë¥¼ ê°€ë…ì„± ì¢‹ì€ í…ìŠ¤íŠ¸ë¡œ ì €ì¥

    Args:
        results: MethodResult ë¦¬ìŠ¤íŠ¸ (agent_trace í¬í•¨)
        output_path: ì €ì¥ ê²½ë¡œ
        method_name: ë°©ë²• ì´ë¦„
    """
    lines = []
    lines.append("=" * 80)
    lines.append(f"Agent Trace Report: {method_name}")
    lines.append(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append("=" * 80)
    lines.append("")

    correct_count = 0
    total_with_trace = 0

    for result in results:
        if result.agent_trace:
            total_with_trace += 1
            trace = result.agent_trace

            # ì •ë‹µ ì—¬ë¶€ í‘œì‹œ
            is_correct = result.ground_truth == result.prediction
            if is_correct:
                correct_count += 1
            status = "âœ… CORRECT" if is_correct else "âŒ WRONG"

            lines.append(f"\n{'#' * 80}")
            lines.append(f"# Sample {result.sample_id}: {result.filename}")
            lines.append(f"# Status: {status}")
            lines.append(f"# Ground Truth: {result.ground_truth}")
            lines.append(f"# Prediction: {result.prediction}")
            lines.append(f"{'#' * 80}")

            # íŠ¸ë ˆì´ìŠ¤ ë‚´ìš© ì¶”ê°€
            lines.append(trace.to_readable_text())

    # ìš”ì•½ ì •ë³´
    summary = f"""
{'=' * 80}
SUMMARY
{'=' * 80}
Method: {method_name}
Total Samples with Trace: {total_with_trace}
Correct: {correct_count}
Accuracy: {correct_count / total_with_trace * 100:.1f}% (of traced samples)
{'=' * 80}
"""
    lines.insert(4, summary)

    with open(output_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(lines))


def save_agent_single_trace(
    trace: AgentTrace,
    output_dir: Path,
    ground_truth: str = ""
) -> None:
    """
    ë‹¨ì¼ ì—ì´ì „íŠ¸ íŠ¸ë ˆì´ìŠ¤ë¥¼ ê°œë³„ íŒŒì¼ë¡œ ì €ì¥

    Args:
        trace: AgentTrace ì¸ìŠ¤í„´ìŠ¤
        output_dir: ì¶œë ¥ ë””ë ‰í„°ë¦¬
        ground_truth: ì •ë‹µ ë¼ë²¨
    """
    # íŒŒì¼ëª… ìƒì„± (ì•ˆì „í•œ ë¬¸ìë§Œ ì‚¬ìš©)
    safe_filename = trace.filename.replace('/', '_').replace('\\', '_')
    json_path = output_dir / f"trace_{trace.sample_id:04d}_{safe_filename}.json"
    txt_path = output_dir / f"trace_{trace.sample_id:04d}_{safe_filename}.txt"

    # JSON ì €ì¥
    trace_dict = trace.to_dict()
    trace_dict["ground_truth"] = ground_truth
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(trace_dict, f, indent=2, ensure_ascii=False)

    # í…ìŠ¤íŠ¸ ì €ì¥
    text_content = trace.to_readable_text()
    text_content = f"Ground Truth: {ground_truth}\n\n" + text_content
    with open(txt_path, 'w', encoding='utf-8') as f:
        f.write(text_content)


def save_all_agent_traces(
    all_results: Dict[str, List[MethodResult]],
    output_dir: Path
) -> None:
    """
    ëª¨ë“  ì—ì´ì „íŠ¸ ê²°ê³¼ì˜ íŠ¸ë ˆì´ìŠ¤ë¥¼ ì €ì¥

    Args:
        all_results: {method_name: [MethodResult, ...]} ë”•ì…”ë„ˆë¦¬
        output_dir: ì¶œë ¥ ë””ë ‰í„°ë¦¬
    """
    agent_methods = ["dermatology_agent", "react_agent"]

    for method_name in agent_methods:
        if method_name not in all_results:
            continue

        results = all_results[method_name]

        # ì—ì´ì „íŠ¸ íŠ¸ë ˆì´ìŠ¤ ë””ë ‰í„°ë¦¬ ìƒì„±
        trace_dir = output_dir / "agent_traces" / method_name
        trace_dir.mkdir(parents=True, exist_ok=True)

        # JSON í†µí•© íŒŒì¼
        json_path = trace_dir / f"{method_name}_all_traces.json"
        save_agent_traces_json(results, json_path, method_name)

        # ê°€ë…ì„± ì¢‹ì€ í…ìŠ¤íŠ¸ íŒŒì¼
        txt_path = trace_dir / f"{method_name}_traces_readable.txt"
        save_agent_traces_readable(results, txt_path, method_name)

        # ê°œë³„ íŠ¸ë ˆì´ìŠ¤ íŒŒì¼ (ì„ íƒì )
        individual_dir = trace_dir / "individual"
        individual_dir.mkdir(parents=True, exist_ok=True)

        for result in results:
            if result.agent_trace:
                save_agent_single_trace(
                    result.agent_trace,
                    individual_dir,
                    result.ground_truth
                )


# ============ ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ ============

def load_dataset(csv_path: str, num_samples: Optional[int] = None) -> pd.DataFrame:
    """
    ë°ì´í„°ì…‹ CSV ë¡œë“œ

    Args:
        csv_path: CSV íŒŒì¼ ê²½ë¡œ
        num_samples: ë¡œë“œí•  ìƒ˜í”Œ ìˆ˜ (Noneì´ë©´ ì „ì²´)

    Returns:
        DataFrame
    """
    df = pd.read_csv(csv_path, encoding='utf-8-sig')

    if num_samples is not None and num_samples < len(df):
        df = df.head(num_samples)

    return df


# ============ ì˜¨í†¨ë¡œì§€ íŠ¸ë¦¬ í…ìŠ¤íŠ¸ ìƒì„± ============

def build_ontology_tree_text(
    ontology: Dict,
    node: str = "root",
    indent: int = 0,
    max_depth: int = 10,
    prefix: str = ""
) -> str:
    """
    ì˜¨í†¨ë¡œì§€ JSONì„ íŠ¸ë¦¬ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜

    Args:
        ontology: ì˜¨í†¨ë¡œì§€ ë”•ì…”ë„ˆë¦¬
        node: í˜„ì¬ ë…¸ë“œ
        indent: ë“¤ì—¬ì“°ê¸° ë ˆë²¨
        max_depth: ìµœëŒ€ ê¹Šì´
        prefix: ì ‘ë‘ì‚¬ (íŠ¸ë¦¬ êµ¬ì¡° í‘œí˜„)

    Returns:
        íŠ¸ë¦¬ í…ìŠ¤íŠ¸
    """
    if indent > max_depth:
        return ""

    lines = []
    children = ontology.get(node, [])

    for i, child in enumerate(children):
        is_last = (i == len(children) - 1)

        # í˜„ì¬ ë…¸ë“œ ì¶œë ¥
        connector = "â””â”€â”€ " if is_last else "â”œâ”€â”€ "
        lines.append(f"{prefix}{connector}{child}")

        # ìì‹ ë…¸ë“œë“¤ì˜ prefix ê²°ì •
        new_prefix = prefix + ("    " if is_last else "â”‚   ")

        # ì¬ê·€ì ìœ¼ë¡œ ìì‹ ë…¸ë“œ ì²˜ë¦¬
        child_tree = build_ontology_tree_text(
            ontology, child, indent + 1, max_depth, new_prefix
        )
        if child_tree:
            lines.append(child_tree)

    return "\n".join(lines)


def get_ontology_tree_for_prompt(ontology_path: Optional[str] = None) -> str:
    """
    í”„ë¡¬í”„íŠ¸ìš© ì˜¨í†¨ë¡œì§€ íŠ¸ë¦¬ í…ìŠ¤íŠ¸ ìƒì„±

    Args:
        ontology_path: ì˜¨í†¨ë¡œì§€ JSON ê²½ë¡œ

    Returns:
        í”„ë¡¬í”„íŠ¸ì— í¬í•¨í•  íŠ¸ë¦¬ í…ìŠ¤íŠ¸
    """
    tree = OntologyTree(ontology_path)
    tree_text = build_ontology_tree_text(tree.ontology, "root", 0, 10, "")
    return tree_text


# ============ ê²°ê³¼ ë¶„ì„ í•¨ìˆ˜ ============

def analyze_method_differences(
    all_results: Dict[str, List[MethodResult]]
) -> Dict[str, Any]:
    """
    ë°©ë²• ê°„ ì°¨ì´ ë¶„ì„

    Args:
        all_results: {method_name: [MethodResult, ...]} ë”•ì…”ë„ˆë¦¬

    Returns:
        ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    method_names = list(all_results.keys())
    if not method_names:
        return {}

    num_samples = len(all_results[method_names[0]])

    # ëª¨ë“  ë°©ë²•ì´ ë§ì¶˜ ìƒ˜í”Œ
    all_correct = []
    # ëª¨ë“  ë°©ë²•ì´ í‹€ë¦° ìƒ˜í”Œ
    all_wrong = []
    # ë°©ë²• ê°„ ì°¨ì´ê°€ ë‚˜ëŠ” ìƒ˜í”Œ
    different = []

    for i in range(num_samples):
        gt = all_results[method_names[0]][i].ground_truth
        preds = [all_results[m][i].prediction for m in method_names]

        correct_count = sum(1 for p in preds if p == gt)

        if correct_count == len(method_names):
            all_correct.append(i)
        elif correct_count == 0:
            all_wrong.append(i)
        else:
            different.append({
                'sample_id': i,
                'ground_truth': gt,
                'predictions': {m: all_results[m][i].prediction for m in method_names},
                'correct_methods': [m for m in method_names if all_results[m][i].prediction == gt]
            })

    return {
        'all_correct_count': len(all_correct),
        'all_wrong_count': len(all_wrong),
        'different_count': len(different),
        'different_samples': different
    }


# ============ ê²°ê³¼ ì¶œë ¥ í•¨ìˆ˜ ============

def print_metrics_summary(evaluations: Dict[str, MethodEvaluation]) -> None:
    """
    ë©”íŠ¸ë¦­ ìš”ì•½ ì¶œë ¥

    Args:
        evaluations: {method_name: MethodEvaluation} ë”•ì…”ë„ˆë¦¬
    """
    print("\n" + "=" * 80)
    print("ë©”íŠ¸ë¦­ ìš”ì•½ (METRICS SUMMARY)")
    print("=" * 80)

    # í—¤ë” ì¶œë ¥
    header = f"{'Method':<25} {'Exact':>8} {'Partial':>8} {'H-F1':>8} {'Dist':>8} {'Credit':>8}"
    print(header)
    print("-" * 80)

    for method_name, eval_result in evaluations.items():
        row = (
            f"{method_name:<25} "
            f"{eval_result.exact_match:>8.4f} "
            f"{eval_result.partial_match:>8.4f} "
            f"{eval_result.hierarchical_f1:>8.4f} "
            f"{eval_result.avg_distance:>8.2f} "
            f"{eval_result.partial_credit:>8.4f}"
        )
        print(row)

    print("=" * 80)

    # ë ˆë²¨ë³„ ì •í™•ë„
    print("\në ˆë²¨ë³„ ì •í™•ë„ (LEVEL ACCURACY)")
    print("-" * 80)

    # ëª¨ë“  ë ˆë²¨ ìˆ˜ì§‘
    all_levels = set()
    for eval_result in evaluations.values():
        all_levels.update(eval_result.level_accuracy.keys())

    if all_levels:
        levels = sorted(all_levels)
        header = f"{'Method':<25}" + "".join([f" L{l}:>8" for l in levels])
        print(f"{'Method':<25}" + "".join([f"{'L'+str(l):>10}" for l in levels]))
        print("-" * 80)

        for method_name, eval_result in evaluations.items():
            row = f"{method_name:<25}"
            for level in levels:
                acc = eval_result.level_accuracy.get(level, 0.0)
                row += f"{acc:>10.4f}"
            print(row)

    # Top-K ì •í™•ë„ (ìˆëŠ” ê²½ìš°)
    has_top_k = any(eval_result.top_k_accuracy for eval_result in evaluations.values())
    if has_top_k:
        print("\n" + "=" * 80)
        print("Top-K ì •í™•ë„ (TOP-K ACCURACY)")
        print("-" * 80)

        # ëª¨ë“  Kê°’ ìˆ˜ì§‘
        all_k_values = set()
        for eval_result in evaluations.values():
            if eval_result.top_k_accuracy:
                all_k_values.update(eval_result.top_k_accuracy.keys())

        if all_k_values:
            k_values = sorted(all_k_values)
            print(f"{'Method':<25}" + "".join([f"{'Top-'+str(k):>12}" for k in k_values]))
            print("-" * 80)

            for method_name, eval_result in evaluations.items():
                row = f"{method_name:<25}"
                for k in k_values:
                    acc = eval_result.top_k_accuracy.get(k, -1)
                    if acc >= 0:
                        row += f"{acc:>12.4f}"
                    else:
                        row += f"{'N/A':>12}"
                print(row)

    print("=" * 80)
